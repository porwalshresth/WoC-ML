For the Kaggle Competition, I have used three algorithms - Logistic Regression, Neural Networks and Boosted Trees (XGBoost)
In the Exploratory Data Analysis, the I first checked if there is any cell in the entire of the dataset which contains a non - numeric value except for the column headers and checked if any row is repeated more than once in the dataset or if any ... I was just curious to know about the class distibution of 0s and 1s in the dataset and so I found out that only about 200 1s exist in the dataset and the remaining 1800 were 0s.
For the implementation of Logistic Regression what I thought that it would work the best if I make a dataset such that it contains a 50:50 ratio of 0s and 1s. I manually copy - pasted the 200 1s 9 more times and trained the algorithm on that dataset and found my model having an accuracy of 0.84 on the public leaderboard - It predicted a total of about 340 1s in the entire of the training dataset and it had the fitting accuaracy to the trained dataset as 1. I used the module sklearn.linear_model to implement the model of logistic regression. I was in a notion that a neural network would be much better in the classification task but was late to realize the same after the Kaggle Competitions results were declared. I focused a lot over the design of neural networks and choosing the parameters of it rather than trying to improvise or improve upon the model of Logistic Regression.

Neural Networks:
I build a total of two neural networks - one was trained with 1600 examples (80%) of the training set given by the club members which were randomly chosen and its accuracy was judged on the cross validation set which had the other 400 examples of the training set; the other one was trained on the entire dataset which used the entire of the dataset on the same parameters as I found from the other neural network.
I used TensorFlow to build the neural network. It had 3 layers containing the units - 128, 64 and 1 respectively.
For the one which was trained on 1600 examples (as described above): 
I first used a loop of 10 random seeds to find out that which gave me the best result on cross - validation set for the value of multiplier of 1s from 1 to 9. I obtained the following result:
<img width="597" height="245" alt="image" src="https://github.com/user-attachments/assets/46efaccf-cca0-4e58-9aa0-95104de287e0" />
I found out that the multiplier which gave me the best result as 6 as it gave me the highest average accuracy.
After founding out the best value of multiplier of 1s, I ran the loop over the values of regularization parameters as 0.0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1 over 10 values of random seed. I obtained the following result:
<img width="602" height="308" alt="Screenshot 2025-12-30 143816" src="https://github.com/user-attachments/assets/3de0f818-7b3b-421d-a2f8-f7c29d79ebc9" />
Since the value of regularization parameter as 1 gave me the best value of average accuracy on the cross - validation set, I decided to check the value of average accuracy on values of regularization parameter as > 1 over the values of 1, 3, 10, 30, 100, 300 which I obtained as follows:
<img width="711" height="240" alt="Screenshot 2025-12-30 150009" src="https://github.com/user-attachments/assets/ec0bc097-759e-42e3-9ab5-6275e110b0f2" />
Since the value of average accuracy for values of regularization parameters â‰¥ 1, I decided to find out the value of mean of all the parameters of the neural network which I found out to be less than 0.001
<img width="666" height="259" alt="image" src="https://github.com/user-attachments/assets/0851e32a-ae7d-48b6-84c6-d458a02cd20c" />
So I found the curve to just have developed a simple model which is learning to predict 0 or I found out that the value of regularization parameter which is sufficiently large so the model becomes so simple that it effectively is underfitting (or has high bias) and is unable to capture the trends in the data.
Considering the fear of fitting and even trying values less than 0.001, I ended up to choose the value of regularization parameter as 0.
