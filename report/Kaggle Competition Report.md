For the Kaggle Competition, I have used three algorithms - Logistic Regression, Neural Networks and Boosted Trees (XGBoost)
In the Exploratory Data Analysis, the I first checked if there is any cell in the entire of the dataset which contains a non - numeric value except for the column headers and checked if any row is repeated more than once in the dataset or if any ... I was just curious to know about the class distibution of 0s and 1s in the dataset and so I found out that only about 200 1s exist in the dataset and the remaining 1800 were 0s.
For the implementation of Logistic Regression what I thought that it would work the best if I make a dataset such that it contains a 50:50 ratio of 0s and 1s. I manually copy - pasted the 200 1s 9 more times and trained the algorithm on that dataset and found my model having an accuracy of 0.84 on the public leaderboard - It predicted a total of about 340 1s in the entire of the training dataset and it had the fitting accuaracy to the trained dataset as 1. I used the module sklearn.linear_model to implement the model of logistic regression. I was in a notion that a neural network would be much better in the classification task but was late to realize the same after the Kaggle Competitions results were declared. I focused a lot over the design of neural networks and choosing the parameters of it rather than trying to improvise or improve upon the model of Logistic Regression.


Neural Networks:
I build a total of two neural networks - one was trained with 1600 examples (80%) of the training set given by the club members which were randomly chosen and its accuracy was judged on the cross validation set which had the other 400 examples of the training set; the other one was trained on the entire dataset which used the entire of the dataset on the same parameters as I found from the other neural network.
I used TensorFlow to build the neural network. It had 3 layers containing the units - 128, 64 and 1 respectively.
For the one which was trained on 1600 examples (as described above): 
I first used a loop of 10 random seeds to find out that which 
<img width="597" height="245" alt="image" src="https://github.com/user-attachments/assets/46efaccf-cca0-4e58-9aa0-95104de287e0" />
